{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b91d42",
   "metadata": {},
   "source": [
    "## Week 5 - Session 2: SSL_AutoEncoder\n",
    "- Download the data under the working directory, using shell script: $ sh download_mnist.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90315b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import numpy as np\n",
    "from load_dataset import mnist\n",
    "\n",
    "import matplotlib.pyplot\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852f9e1",
   "metadata": {},
   "source": [
    "### Fully-connected network (feed-forward network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319f542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_networks(epochs, learning_rate):\n",
    "    \n",
    "    # input layer: size of digit image (784 = 28 x 28)\n",
    "    x_fc = x = tf.compat.v1.placeholder(tf.float32, [None, 784]) \n",
    "\n",
    "    # hidden layer: 200 hidden units\n",
    "    W1_fc = tf.compat.v1.Variable(tf.random.normal([784, 200], stddev=0.03), name='W1_FC')\n",
    "    b1_fc = tf.compat.v1.Variable(tf.random.normal([200]), name='b1_fc')\n",
    "    l1_linear_output = tf.add(tf.matmul(x_fc, W1_fc), b1_fc) \n",
    "    l1_activation = tf.compat.v1.nn.relu(l1_linear_output) \n",
    "    \n",
    "    # output layer: multi-classes (10 digits)\n",
    "    W2_fc = tf.compat.v1.Variable(tf.random.normal([200, 10], stddev=0.03), name='W2_FC')\n",
    "    b2_fc = tf.compat.v1.Variable(tf.random.normal([10]), name='b2_fc')  \n",
    "    # softmax: probability distribution of output classes\n",
    "    y_fc = tf.compat.v1.nn.softmax(tf.add(tf.matmul(l1_activation, W2_fc), b2_fc))  \n",
    "\n",
    "    # cost function\n",
    "    y_label_input_fc = tf.compat.v1.placeholder(tf.float32, [None, 10]) # true labels\n",
    "    cross_entropy_fc = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_label_input_fc, logits=y_fc))\n",
    "    \n",
    "    # Training with an optimizer\n",
    "    fc_classifier = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                        beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(cross_entropy_fc)\n",
    "    \n",
    "    return x_fc, y_label_input_fc, y_fc, cross_entropy_fc, fc_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943eff6a",
   "metadata": {},
   "source": [
    "### Looking at the above function, answer the following questions:\n",
    "\n",
    "1. What is a feed-forward network.  Explain in a couple of sentences.\n",
    "2. What are some advantages to a feed-forward network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754508ef",
   "metadata": {},
   "source": [
    "### (Unsupervised) Sparse autoencoder  + (Supervised) NN Classifier = (Semi-Supervised) NN classifier\n",
    "* Autoencoder is an unsupervised learning technique for neural networks to automatically learn features from unlabeled data.\n",
    "* Sparse constraints can significantly save computing resources and find the characteristics of data in a low-dimensional space\n",
    "\n",
    "--> Sparse autoencoder =  efficiently train latent features from unlabeled data in a low dimensional space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e49ba",
   "metadata": {},
   "source": [
    "### Sparse autoecoder with classification: High-level Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd9fcc",
   "metadata": {},
   "source": [
    " 1. (unsupervised) Train latent representation (extracted features) from all the data (labeled + unlabeled)\n",
    "   - Encoding: the extracted (and abstracted) features from the whole data (L+U) are more general than the features only from labeled data (L).\n",
    "   - Decoding: make sure the decoded output from latent representation close to the input\n",
    " 2. (supervised) Train a classifier using the latent representation and labels from labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ed22f",
   "metadata": {},
   "source": [
    "### AutoEncoder Network Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c905cf",
   "metadata": {},
   "source": [
    "input (x) -- Encoder (hidden layer) -- [latent features] -- Decorder (x_hat)\n",
    " \n",
    "                                      \\ Classifier (y_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24808c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_autoEncoder(learning_rate, reg_term_lambda, p, beta):\n",
    "    \n",
    "    # 1. train an unsupervised auto-encoder on the whole training data (L + U)\n",
    "    # x: input, x_hat: decoded output\n",
    "    # encoded variables: latent representation \n",
    "    x = tf.compat.v1.placeholder(tf.float32, [None, 784]) # 784 = 28 * 28 (size of a digit image)\n",
    "    x_hat = tf.compat.v1.placeholder(tf.float32, [None, 784])\n",
    "    y_label_input = tf.compat.v1.placeholder(tf.float32,[None,10])\n",
    "\n",
    "    # Encoder: First hidden layer\n",
    "    W1 = tf.compat.v1.Variable(tf.random.normal([784, 200], stddev=0.03), name='W1')  # num of hidden unit: 200\n",
    "    b1 = tf.compat.v1.Variable(tf.random.normal([200]), name='b1')\n",
    "    # Output from hidden layer 1\n",
    "    linear_layer_one_output = tf.add(tf.matmul(x, W1), b1)\n",
    "    layer_one_output = tf.compat.v1.nn.sigmoid(linear_layer_one_output) # Activation function: sigmoid\n",
    "\n",
    "    # Decorder\n",
    "    W2 = tf.compat.v1.Variable(tf.random.normal([200, 784], stddev=0.03), name='W2')  # output size = input size\n",
    "    b2 = tf.compat.v1.Variable(tf.random.normal([784]), name='b2')\n",
    "    # x_hat : reconstructed output from latent representation \n",
    "    linear_layer_two_output = tf.add(tf.matmul(layer_one_output, W2), b2)\n",
    "    x_hat = tf.compat.v1.nn.sigmoid(linear_layer_two_output)\n",
    "\n",
    "    # Softmax classifier weight initialization\n",
    "    W3 = tf.compat.v1.Variable(tf.random.normal([200, 10], stddev=0.03), name='W3')   # \n",
    "    b3 = tf.compat.v1.Variable(tf.random.normal([10]), name='b3')\n",
    "\n",
    "    # 2. Connect softmax with feature extractor:\n",
    "    # Use the latent representations extracted from this auto-encoder \n",
    "    # to train a softmax classier on labeled proportion of data.\n",
    "    y_label = tf.compat.v1.nn.softmax(tf.add(tf.matmul(layer_one_output, W3), b3))\n",
    "\n",
    "    \n",
    "    # Define classifier\n",
    "    cross_entropy = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_label_input, logits=y_label))\n",
    "    \n",
    "    softmax_classifier = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, \n",
    "                                                          beta2=0.999, epsilon=1e-08).minimize(cross_entropy)\n",
    "    \n",
    "    # p_hat : latent representation  \n",
    "    p_hat = tf.reduce_mean(tf.compat.v1.clip_by_value(layer_one_output, 1e-10, 1.0), axis=0)\n",
    "\n",
    "    # p_hat = tf.reduce_mean(layer_one_output,axis=1)\n",
    "    kl = kl_divergence(p, p_hat)\n",
    "\n",
    "    # Define the cost function for the decoder\n",
    "    # 1) diff : difference between reconstruected output x_hat and input x\n",
    "    # 2) reg_term_lambda (weight decay parameter): decrease the magnitude of the weights, \n",
    "    #                                           and helps prevent overfitting\n",
    "    # 3) beta: controls the weight of the sparsity penalty term\n",
    "    diff = x_hat - x   \n",
    "    cost = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis=1)) + reg_term_lambda * (\n",
    "                tf.compat.v1.nn.l2_loss(W1) + tf.compat.v1.nn.l2_loss(W2)) + beta * tf.reduce_sum(kl)\n",
    "\n",
    "    optimiser = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, \n",
    "                                                 epsilon=1e-08).minimize(cost)\n",
    "\n",
    "    init_op = tf.compat.v1.global_variables_initializer()\n",
    "    \n",
    "    return x, y_label_input, y_label, cost, optimiser, init_op, cross_entropy, softmax_classifier\n",
    "    \n",
    "    \n",
    "# KL divergence: a standard function for measuring how different two different distributions are.\n",
    "# if KL == 0, p = p_hat\n",
    "# otherwise, it increases monotonically as p_hat diverges from p.\n",
    "def kl_divergence(p, p_hat):\n",
    "    return p * tf.math.log(p) - p * tf.math.log(p_hat) + (1 - p) * tf.math.log(1 - p) - (1 - p) * tf.math.log(1 - p_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ffb04",
   "metadata": {},
   "source": [
    "### Looking at the code above, answer the following questions:\n",
    "\n",
    "1. What is the autoencoder doing?\n",
    "2. Why is it necessary to use this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2a6d3",
   "metadata": {},
   "source": [
    "### Data preperation\n",
    " - get a subset of 100 data for each digit as labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22b24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Subset_LabeledData(train_data, train_label, test_data, test_label):\n",
    "    digits = [0,0,0,0,0,0,0,0,0,0]\n",
    "    index_to_get = []\n",
    "    # Get the subset dataset: 100 data for each digit\n",
    "    index_l = 0\n",
    "    for i in range(0,train_label.shape[1]):\n",
    "        #print (train_label[0][i])\n",
    "        if digits[int(train_label[0][i])] == 100:\n",
    "            continue\n",
    "        else:\n",
    "            digits[int(train_label[0][i])]+=1\n",
    "            index_to_get.append(index_l)\n",
    "\n",
    "        index_l += 1\n",
    "\n",
    "    # Labeled samples    \n",
    "    train_data_new = train_data.take(index_to_get,axis=1)\n",
    "    train_label_new = train_label.take(index_to_get)\n",
    "\n",
    "    test_label_new = test_label[0]\n",
    "\n",
    "    n_values = np.max(train_label_new.astype(int)) + 1              # number of digits\n",
    "    train_label_new = np.eye(n_values)[train_label_new.astype(int)] # one-hot encoded labels\n",
    "\n",
    "    n_values = np.max(test_label_new.astype(int)) + 1               # number of digits\n",
    "    test_label_new = np.eye(n_values)[test_label_new.astype(int)]   # one-hot encoded labels\n",
    "    \n",
    "    return train_data_new, train_label_new, test_data, test_label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28cf111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fc(epochs, ntrain):\n",
    "    \n",
    "    # Load the data\n",
    "    train_data, train_label, test_data, test_label = mnist(ntrain, ntest=1000, digit_range=[0, 10])\n",
    "    \n",
    "    # Select 100 labeled sample for each digit from the training data (L), \n",
    "    # and use the rest of the training set as the unlabeled data (U).    \n",
    "    train_data_new, train_label_new, test_data_new, test_label_new = get_Subset_LabeledData(train_data, train_label, \n",
    "                                                                                      test_data, test_label)\n",
    "\n",
    "    print (\"labeled data: train {}, test {}\".format(train_label_new.shape, test_label_new.shape))\n",
    "\n",
    "    # Define a fully-connected networks\n",
    "    x_fc, y_label_input_fc, y_fc, cross_entropy_fc, fc_classifier = fc_networks(epochs, learning_rate=1e-3)   \n",
    "\n",
    "    # Define a sparse autoencoder\n",
    "    x, y_label_input, y_label, cost, optimiser, init_op, cross_entropy, softmax_classifier = sparse_autoEncoder(\n",
    "        learning_rate=1e-3, reg_term_lambda=1e-3, p=0.1, beta=3)\n",
    "    \n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)\n",
    "\n",
    "        # FC\n",
    "        print(\"Training FC network using the labeled data (L)\")\n",
    "        for epoch in range(epochs):\n",
    "            cost_fc = 0\n",
    "            _ , cost_fc = sess.run([fc_classifier, cross_entropy_fc],\n",
    "                                   feed_dict={x_fc: train_data_new.T, \n",
    "                                              y_label_input_fc: train_label_new})\n",
    "            if ((epoch + 1) % 200 == 0):\n",
    "                print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.3f}\".format(cost_fc))\n",
    "                \n",
    "        correct_prediction_fc = tf.equal(tf.argmax(y_label_input_fc, 1), tf.argmax(y_fc, 1))\n",
    "        accuracy_fc = tf.reduce_mean(tf.cast(correct_prediction_fc, tf.float32))                \n",
    "        print(\"\\nAccuracy of FC network is\", sess.run(accuracy_fc, \n",
    "                                        feed_dict={x_fc: test_data_new.T, \n",
    "                                                   y_label_input_fc: test_label_new}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f1cd0",
   "metadata": {},
   "source": [
    "### Train AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea810b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(epochs, ntrain):\n",
    "    \n",
    "    # Load the data\n",
    "    train_data, train_label, test_data, test_label = mnist(ntrain, ntest=1000, digit_range=[0, 10])\n",
    "    \n",
    "    # Select 100 labeled sample for each digit from the training data (L), \n",
    "    # and use the rest of the training set as the unlabeled data (U).    \n",
    "    train_data_new, train_label_new, test_data_new, test_label_new = get_Subset_LabeledData(train_data, train_label, \n",
    "                                                                                      test_data, test_label)\n",
    "\n",
    "    print (\"labeled data: train {}, test {}\".format(train_label_new.shape, test_label_new.shape))\n",
    "\n",
    "    # Define a fully-connected networks\n",
    "    x_fc, y_label_input_fc, y_fc, cross_entropy_fc, fc_classifier = fc_networks(epochs, learning_rate=1e-3)   \n",
    "\n",
    "    # Define a sparse autoencoder\n",
    "    x, y_label_input, y_label, cost, optimiser, init_op, cross_entropy, softmax_classifier = sparse_autoEncoder(\n",
    "        learning_rate=1e-3, reg_term_lambda=1e-3, p=0.1, beta=3)\n",
    "    \n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # initialise the variables\n",
    "        sess.run(init_op)             \n",
    "\n",
    "        print(\"\\nTraining Sparse Autoencoder using the whole data (L + U)\")\n",
    "        for epoch in range(epochs):\n",
    "            _, c = sess.run([optimiser, cost], feed_dict={x: train_data.T})            \n",
    "            if((epoch+1)%200 == 0):\n",
    "                print(\"Epoch:\", (epoch + 1), \"cost = {:.3f}\".format(c))\n",
    "\n",
    "        print(\"Training a softmax classifier with Autoencoder latent representations, using the labeled data (L)\")\n",
    "        for epoch in range(epochs):\n",
    "            cost = 0\n",
    "            _ , cost = sess.run([softmax_classifier, cross_entropy], feed_dict={x: train_data_new.T, y_label_input : train_label_new})\n",
    "            if ((epoch + 1) % 200 == 0):\n",
    "                print(\"Epoch:\", (epoch + 1), \"cost = {:.3f}\".format(cost))\n",
    "\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(y_label_input,1), tf.argmax(y_label,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        print(\"Accuracy of Sparse Autoencoder is\", sess.run(accuracy, \n",
    "                                      feed_dict={x: test_data_new.T, y_label_input : test_label_new}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee8f3e",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1. What is the cost telling us?\n",
    "2. What is the epoch telling us?\n",
    "3. Compare the prediction performance between Supervised Fully-connected networks and Semi-supervised AutoEncoder.\n",
    "4. Change the number of unlabeled data, and observe the results\n",
    "   - The labeled data is fixed with 1000 instances.\n",
    "   - To change the number of unlabeld data, change 'ntrain'. (i.e., unlabeled data num = ntrain - 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98023ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled data: train (1000, 10), test (1000, 10)\n",
      "Training FC network using the labeled data (L)\n",
      "Epoch: 200 cost = 1.489\n",
      "Epoch: 400 cost = 1.479\n",
      "Epoch: 600 cost = 1.477\n",
      "Epoch: 800 cost = 1.475\n",
      "Epoch: 1000 cost = 1.474\n",
      "\n",
      "Accuracy of FC network is 0.878\n",
      "labeled data: train (1000, 10), test (1000, 10)\n",
      "\n",
      "Training Sparse Autoencoder using the whole data (L + U)\n",
      "Epoch: 200 cost = 55.740\n",
      "Epoch: 400 cost = 38.836\n",
      "Epoch: 600 cost = 31.550\n",
      "Epoch: 800 cost = 27.274\n",
      "Epoch: 1000 cost = 24.449\n",
      "Training a softmax classifier with Autoencoder latent representations, using the labeled data (L)\n",
      "Epoch: 200 cost = 1.514\n",
      "Epoch: 400 cost = 1.473\n",
      "Epoch: 600 cost = 1.469\n",
      "Epoch: 800 cost = 1.468\n",
      "Epoch: 1000 cost = 1.468\n",
      "Accuracy of Sparse Autoencoder is 0.881\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    epochs = 1000\n",
    "    ntrain = 6000\n",
    "    train_fc(epochs, ntrain)\n",
    "    train_autoencoder(epochs, ntrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a441c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
