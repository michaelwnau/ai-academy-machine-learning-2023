{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMsYoaVJH43mvxT8ipwSJc8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwnau/ai-academy-machine-learning-2023/blob/main/hmm_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction to Value Iteration in Reinforcement Learning\n",
        "This notebook implements a basic form of value iteration, a key algorithm in reinforcement learning. Value iteration is used to compute the optimal policy for a given reward structure in a Markov decision process. It iteratively updates the value of each state to find the optimal policy."
      ],
      "metadata": {
        "id": "4HdsQsEY0oEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Setup: Defining the number of iterations and initial rewards for different states/actions\n",
        "iterations = 500\n",
        "PU_rew = 0\n",
        "PF_rew = 0\n",
        "RU_rew = 10\n",
        "RF_rew = 10"
      ],
      "metadata": {
        "id": "XzM4VpEq0dl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying Initial Rewards\n",
        "First, let's display the initial rewards for each state/action. These values will be updated through the iteration process."
      ],
      "metadata": {
        "id": "9jhkwPzZ0x2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f' PU    PF    RU     RF')\n",
        "print(f'{PU_rew : .2f} {PF_rew : .2f} {RU_rew : .2f} {RF_rew: .2f}')"
      ],
      "metadata": {
        "id": "7EPNs0cl0iuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Value Iteration Loop\n",
        "In this section, we perform the value iteration. In each iteration, we calculate the value for each state under different actions or conditions. The calculation considers both the current reward and an estimated future reward, discounted by a factor (0.9 in this example). The future reward is estimated based on the probability of transitioning to other states. The max function is used to select the action with the highest value for each state, updating the rewards for the next iteration."
      ],
      "metadata": {
        "id": "DA5rgwXI0_qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration in range(iterations):\n",
        "    # Calculating values for each state/action combination\n",
        "    PU_sav = 0 + .9*1*PU_rew\n",
        "    PU_Ad = 0 + .9*(.5*PU_rew + .5*PF_rew)\n",
        "    PF_sav = 0 + .9*(.5*PU_rew + .5*RF_rew)\n",
        "    PF_Ad = 0 + .9*(1*PF_rew)\n",
        "    RU_sav = 10 + .9*(.5*RU_rew + .5*PU_rew)\n",
        "    RU_Ad = 10 + .9*(.5*PU_rew + .5*PF_rew)\n",
        "    RF_sav = 10 + .9*(.5*RU_rew + .5*RF_rew)\n",
        "    RF_Ad = 10 + .9*(PF_rew)\n",
        "\n",
        "    # Updating rewards based on calculated values\n",
        "    PU_rew = max(PU_sav, PU_Ad)\n",
        "    PF_rew = max(PF_sav, PF_Ad)\n",
        "    RU_rew = max(RU_sav, RU_Ad)\n",
        "    RF_rew = max(RF_sav, RF_Ad)\n",
        "\n",
        "    # Printing updated rewards\n",
        "    print(f'{PU_rew : .2f} {PF_rew : .2f} {RU_rew : .2f} {RF_rew: .2f}')\n"
      ],
      "metadata": {
        "id": "a__T2JC11ESM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "As the iteration progresses, the printed values show how the estimated rewards for each state/action combination evolve. The algorithm aims to converge to an optimal set of values, representing the best possible policy under the given reward structure and transition probabilities."
      ],
      "metadata": {
        "id": "syuYM7jK1IvK"
      }
    }
  ]
}